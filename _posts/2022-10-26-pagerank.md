---
title: 'PageRank, Stochastic Matrices and the Power Iteration'
date: 2022-10-7
permalink: /posts/pagerank
toc: true
---

In this post, we will revisit a popular algorithm called [PageRank](https://en.wikipedia.org/wiki/PageRank), which is used by Google to rank webpages for its search engine. Surprising to some but not so to others, PageRank is simple enough that only a level of first-year undergraduate linear algebra is required to understand it.

## The Web as a Graph

Consider the web as a collection of *pages*, some of which are connected to each other using *hyperlinks*. For example, the Wikipedia article on [general relativity](https://en.wikipedia.org/wiki/General_relativity) contains a hyperlink to another article on [Albert Einstein](https://en.wikipedia.org/wiki/Albert_Einstein). By clicking the link, we move from the former webpage to the latter.

We can model this as a graph $$G = (V, E)$$, where the set of nodes (or vertices) $$V$$ contains the webpages and the set of edges $$E$$ contains binary relations $$(v_i, v_j)$$, indicating that the page $$v_i \in V$$ contains a hyperlink to $$v_j \in E$$. Since $$v_i$$ may lead to $$v_j$$ but not the other way around, the edge $$(v_i, v_j)$$ may be in $$E$$ while $$(v_j, v_i)$$ may not. In this case we call the graph $$G$$ a *directed* graph.

## The Importance of a Page

PageRank defines a score for each webpage where more "important" pages have high scores. This is particularly useful in *information retrieval*, where a system is asked to return pages relevant to a query. An assumption is that higher-ranked pages should be returned first, as they are more important and therefore have a higher chance of being what the user wants. Some other intuitions on building a ranking system are:
- If many pages have a hyperlink to page $$i$$, then $$i$$ should be important.
- If a highly ranked page links to page $$i$$, then $$i$$ should also be highly ranked.

Let $$r \in \mathbb{R}^n$$ be the rank vector---that is, $$r_i$$ is the numerical value denoting the importance of of page $$i$$. We will propose a method for finding $$r$$ such that if $$r_i > r_j$$, then page $$i$$ is more important than page $$j$$. Note that since the rankings are ordinal, we can just scale $$r$$ by a positive number and the relative positions of the pages will not change at all. The design below should be able to handle this.

### The importance matrix

To find the importance of every page, we will need to exploit the structure of the graph, specifically the in- and out-links of every node. Suppose that page $$i$$ with importance $$r_i$$ has $$d_i$$ out-neighbors---that is, pages that $$i$$ links to. In graph theory, $$d_i$$ is also called the *out-degree* of $$i$$. Based on the intuitions above, we want these out-neighbors to enjoy $$i$$'s importance. To do so, we assume that each out-neighbor of $$i$$ will get an equal amount of importance from $$i$$. In other words, each out-neighbor will get an amount $$\frac{r_i}{d_i}$$ of importance from $$i$$.

In this setting, the importance of a page $$j$$ will be the sum of all importance flowing into it from its in-neighbors:

$$
\begin{align} \label{eq:importance_flow}
    r_j = \sum_{i \rightarrow j} \frac{r_i}{d_i}.
\end{align}
$$

Notice that we have a recursive structure: Every page influences the pages it leads to. But the importance of that page is flows from the pages leading to it. 

Define a matrix $$A$$, called the *importance matrix*, where $$A_{j, i} = \frac{1}{d_i}$$ if page $$i$$ leads to page $$j$$. In other words, each column of $$i$$ of $$A$$ is a vector containing either $$0$$ (where there is no out-going edge) or $$\frac{1}{d_i}$$ (when there is). Since the out-degree of $$i$$ is exactly $$d_i$$, it must be the case that every column of $$i$$ sums to $$1$$.

The product $$A r$$ gives us the importance flowing into every page. To see why it is, consider the $$j$$th component of this product:

$$
\begin{align*}
    (A r)_j = \sum_{i=1}^{n} A_{j, i} r_{i} = \sum_{i \rightarrow j} \frac{r_i}{d_i},
\end{align*}
$$

where we have the last inequality because $$A_{j, i}$$ is non-zero (and equal to $$\frac{1}{d_i}$$) when there is an edge from $$i$$ to $$j$$. This equation exactly matches $$\eqref{eq:importance_flow}$$.

### PageRank as a fixed-point problem

Since $$(A r)_j$$ gives us the importance of page $$j$$, which is also equal to $$r_i$$, we have:

$$
\begin{align} \label{eq:fixed_point}
    A r = r.
\end{align}
$$

The solution $$r$$ to this linear system is the vector containing the ranks of our webpages. Note that we can scale $$r$$ by a positive number and it would still satisfy this equation, achieving our goal of preserving the order from positive scaling stated above.

Such an $$r$$ satisfying $$\eqref{eq:fixed_point}$$ is called a *fixed point* of $$A$$, because applying $$A$$ to $$r$$ (that is, multiplying $$A$$ by $$r$$) will not change the values of $$r$$ at all. I have another post on solving for a fixed point in the context of machine learing, which can be found [here](/posts/anderson-acceleration). In this post, we will revisit a method to solve for $$r$$.


## Solving PageRank

If we look again at equation $$\eqref{eq:fixed_point}$$, we can recognize that this is an eigenvector problem. Specifically, if $$\eqref{eq:fixed_point}$$ holds, then $$r$$ must be an eigenvector of $$A$$ corresponding to an eigenvalue of $$1$$. There are two important questions to answer.

First, is it guaranteed that $$A$$ has $$1$$ as an eigenvector? After all, $$A$$ is just a non-negative matrix with each column summing to $$1$$. It turns out that this is true, and we will see the proof below.

Second, given that $$1$$ is an eigenvector, then we can solve $$A r = r$$ using a row-reduction algorithm such as [Gaussian elimination].(https://en.wikipedia.org/wiki/Gaussian_elimination) Is that it? The answer is no, because Gaussian elimination has a time complexity of $$O(n^3)$$, where $$n$$ is the number of pages. This does not scale well with our page collection, because $$n$$ could be in the billions, if not more. Therefore, we need to find another way to solve $$\eqref{eq:fixed_point}$$.

### Stochastic matrices

To answer the first question above, notice that the matrix $$A$$ is an example of a *stochastic matrix*, which is a square matrix with non-negative entries and having every column sum to 1. In the context of PageRank, $$A$$ is also called the *stochastic adjacency matrix*.

What is interesting about a stochastic matrix is that it accepts $$1$$ as an eigenvalue, and all other eigenvalues (real or complex) of $$A$$ are less than or equal to $$1$$ in absolute value.


<details>
  <summary>Proof</summary>
    <div style="padding-left:1em">
        Since $A$ is a square matrix, $A$ and $A^\top$ share the same eigenvalues. So we need to prove that $1$ is an eigenvalue of $A^\top$. Because every row of $A^\top$ sums to $1$, we have $A^\top \mathbf{1} = \mathbf{1}$, where $\mathbf{1}$ is a vector of all ones. Therefore, $1$ is an eigenvalue of $A^\top$ and $A$.

        <br/><br/>

        To show why all other eigenvalues of $A$ are less than or equal to $1$ in absolute value, let $\lambda$ be an eigenvalue of $A$. So $\lambda$ is also an eigenvalue of $A^\top$, associated with an eigenvector $x = [x1,\ldots,x_n]^\top$. In other words, $A^\top x = \lambda x$. Let $j$ be index of the largest element of $x$, that is, $|x_i| \leq |x_j| ~ \text{for all} i=1,\ldots,n$. We have
        $$
        \begin{align*}
        |\lambda| |x_j| = |\lambda x_j| = \left| \sum_{i=1}^{n} A_{i, j} x_i \right| \leq \sum_{i=1}^{n} A_{i, j} |x_j| = |x_j| \sum_{i=1}^{n} A_{i, j} = |x_j|,
        \end{align*}
        $$
        which implies that $|\lambda| \leq 1$.
    </div>
</details>

### The fixed-point iteration

To answer the second question, we use the fact we just proved above, which is that $$1$$ is the largest eigenvalue of $$A$$ in absolute value. In linear algebra, it is also called the [*spectral radius*](https://en.wikipedia.org/wiki/Spectral_radius) of $$A$$. As an alternative to Gaussian elimination, a popular algorithm to find the spectral radius and its corresponding eigenvector is the [power iteration](https://en.wikipedia.org/wiki/Power_iteration).

<div style='padding-left:2em; padding-bottom:1em'>
Procedure: Power Iteration
<br/>
Input: A diagonalizable $n \times n$ matrix $A$
<br/>
Let $b_0$ some non-zero vector
<br/>
For $k = 0, \ldots, K-1$ do
<br/>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Apply $A$ to $b_k$: $\tilde{b}_{k+1} = A b_{k}$
<br/>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Normalize: $b_{k+1} = \frac{\tilde{b}_{k+1}}{\lVert \tilde{b}_{k+1} \rVert}$
<br/>
Output: $b_{K}$
</div>

The sequence $$\left(\frac{\lVert A b_k \rVert}{\lVert b_k \rVert}\right)_k$$ is guaranteed to converge to the spectral radius of $$A$$ (which is $$1$$ in our case), and the sequence $$(b_k)_k$$ converges to the corresponding eigenvector with unit norm.

The convergence rate of this sequence can be found [here](https://en.wikipedia.org/wiki/Power_iteration#Analysis). In practice, one would run the power iteration until the difference between two iterates falls below some pre-defined tolerance $$\epsilon$$; for example, until $$\lVert b_{k-1} - b_{k} \leq \rVert 10^{-3}$$.

### Conclusion (temporary)

We have learned how to find the importance scores of webpages in order to rank them. First, we construct the importance matrix from the structure of the graph. Then, we use the power iteration to solve for the fixed point of this matrix, which is the eigenvector corresponding to the largest eigenvalue in absolute value. This solution $$r$$ now contains the importance of the pages, and we are ready to use $$r$$ to rank them!